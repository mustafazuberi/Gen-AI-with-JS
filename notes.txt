1 - What is Generative AI?

Generative AI (Gen AI) is a type of artificial intelligence designed to create new content. By content, we mean text, images, videos, voice, music, code, and more.

Traditionally, AI models were trained to analyze or classify data‚Äîfor example, detecting objects in images or categorizing text. Generative AI goes a step further: it generates new content that is similar to the data it was trained on, effectively creating something original while learning from existing patterns.

So inshort the generation of content using AI is called generative AI.

2 - What are LLMs?
LLMs, or Large Language Models, are AI models designed specifically for generating and understanding text. They can write, summarize, translate, or answer questions by learning patterns from vast amounts of text data.


3 ‚Äì What did we do before LLMs existed?

Before LLMs, we used statistical models. These models looked at the previous few words and guessed the next word using word frequency and probability. They were sometimes accurate, but very limited.

To improve, we started using neural networks and deep learning. One type we used was called Recurrent Neural Networks (RNNs). RNNs could generate better quality text than statistical models, but they had a limited memory. This meant they could only understand short contexts, not long ones, so they weren‚Äôt perfect.

Then, Google published a research paper called ‚ÄúAttention is All You Need‚Äù. They introduced a new architecture called the Transformer, which could look at all words in a sentence at the same time. This made text generation much more powerful and accurate.

Based on this research, new models were created, like GPT models and BERT models. The Transformer architecture became a game-changer in AI.

These models don‚Äôt just start predicting words on their own‚Äîthey are trained on massive amounts of data, like Wikipedia, books, and information available on the internet.

When we say a model has 8 billion parameters, or another has 32 billion, it refers to the weights of the neural network. More parameters usually mean the model is bigger and more powerful, while fewer parameters make it lighter and faster. Some of today‚Äôs state-of-the-art models are trained on trillions of parameters, which makes them extremely capable.


4 - Models and there capabilities?
There aren‚Äôt official categories for AI models, but we can group them to understand their capabilities:

1. GPT Models:
These models generate outputs immediately when they get input.
They don‚Äôt perform a step-by-step ‚Äúthinking‚Äù process.
This makes GPT models very fast at generating text.

2. Reasoning Models:
These models think through the problem before giving an output.
For example, if you ask the model to generate an image or solve a complex task, it analyzes the input step by step before producing the result.
Because of this thought process, reasoning models are slower than GPT models, but they can handle more complex tasks.

When to use which:
Just because GPT models are fast doesn‚Äôt mean we always use them.
For simple tasks, GPT models are fine.
For complex tasks or multi-step planning, reasoning models are better.
In agentic systems, sometimes a ‚Äúrouter‚Äù is used‚Äîa decision-making point where the model decides which path to take.
In these cases, the model needs reasoning power, so reasoning models are preferred.


5 - Distil Model?

A distil model is a smaller version of a big model.
The big model acts as a teacher, sharing its knowledge.
The small model learns from the big model, trying to copy its behavior.
This process of transferring knowledge is called distillation.

Why use it:
The small model is faster and lighter.
It can do most of what the big model does, but with less computing power.


6 - Tokens? 

A token is the smallest piece of text that an LLM can understand.
When you type something like:

üëâ "write me a poem"
It doesn‚Äôt go directly into the model. First, it passes through a step called tokenization.
A special tool called a tokenizer breaks the sentence into tokens.
A token can be a whole word or a part of a word (depends on how the tokenizer works).
Each token is then converted into a number (an ID).

Why? Because:
‚û°Ô∏è LLMs don‚Äôt understand text. They only understand numbers.
So, the text ‚Üí tokens ‚Üí numbers ‚Üí then embeddings ‚Üí then into the neural network.

Example:
"write me a poem" might become tokens like:
["write", " me", " a", " poem"]
and then numbers like:
[1509, 502, 257, 10503]

This is how your text is translated into something the model can actually process.

Why tokens matter:
We need to understand tokens because when we use APIs in Generative AI, the pricing is based on the number of tokens processed. Both the input tokens (your prompt) and the output tokens (the model‚Äôs reply) count toward usage. So, if a company says ‚Äú1 million tokens for $1,‚Äù it means that once your total processed tokens reach one million, you will be charged $1.



Context?
The surrounding text or information the model uses to understand and generate relevant responses.
Models do have a lot of knowledge, but they don‚Äôt have our personal information. For example, if we are building an agentic system for a company, the model doesn‚Äôt know the company‚Äôs internal information. We can provide that information to the model, and this approach is called RAG (Retrieval-Augmented Generation).

Along with that, there‚Äôs also something called message history. When we chat with an AI, by design the model is stateless‚Äîmeaning if we ask a question and it answers, it won‚Äôt remember that conversation in the next prompt. To solve this limitation, whenever we send input, we also include the message history.

All of this information we provide to the LLM is called context.


Context Window?
The maximum number of tokens an LLM can read and use at the same time to generate or predict text. An LLM also has limitations‚Äîit‚Äôs not like we can give it as much input as possible. The context size is called the context window, and it is determined by the token limitation we have. Just like that, every model has its own context window.

We always have to select models based on our requirements‚Äîhow much context size we need or how much output size we might require, since output tokens are also counted and limited. Another important term is knowledge cutoff. Whenever we use models, we often see in the documentation a mentioned date called the knowledge cutoff. This means that the model was trained on data up until that date, and after that point, it doesn‚Äôt have knowledge of new events or information.


Inference?
The process of an LLM taking input and producing output is called inference, and that‚Äôs the stage where users actually interact with the model. Training happens once (or in stages when models are fine-tuned), but inference happens millions or even billions of times as people use the model. That‚Äôs why today‚Äôs competition among cloud providers (like OpenAI, Anthropic, Google, Microsoft, Amazon, etc.) is largely about how fast, efficient, and cost-effective their inference is.

The race isn‚Äôt only about who has the biggest or most capable model‚Äîit‚Äôs also about who can deliver low-latency inference at scale (so users get instant responses without huge costs). This is why you see a lot of work around model optimization, GPU/TPU efficiency, quantization, batching, and distributed inference systems.

So when we select a model, we also need to consider its inference speed. If our application requires fast responses, we must ensure the model provides low-latency inference, which is usually mentioned in its specifications. Larger models typically take more time compared to smaller ones, and reasoning models can be slower as well because of the additional steps involved.


Prompt Engineering?
Prompt Engineering is one of the most important concepts in generative AI if we truly want to build agents and AI systems effectively.

The problem is that models are not deterministic, which means if we give the same input now, the output might be something different just a minute later. The output is not fixed, and the model does not always provide consistency. But when we build systems, we always need consistent outputs, because we don‚Äôt want to break other programming logic in our system.

Prompt engineering is simply a technique to improve the capacity of LLMs. A prompt is just the text we send to an LLM. For example, while using ChatGPT directly, the text we send is very simple. But when we use APIs, we often create prompts using a special structure with multiple elements:

Instructions: Tell the LLM what it needs to do (e.g., translation, summarization).
Input Data: The actual data on which the instruction should be performed.
Context: Additional information that helps the LLM understand better.
Output Indicator: Tells the LLM how or from where the output should start.

Example
Prompt:
You are an English grammar assistant. 
Fix grammar: "i is mustafa zuberi" 
Give only the corrected sentence as output.

Context ‚Üí You are an English grammar assistant.
Instruction ‚Üí Fix grammar
Input Data ‚Üí "i is mustafa zuberi"
Output Indicator ‚Üí Give only the corrected sentence as output.


Zero-shot prompting?
Zero-shot prompting is when we don‚Äôt give any examples to the LLM. We directly ask a question or give an instruction in the prompt, and the model tries to generate the answer based on its knowledge.

e.g: Translate this sentence into French: "I am learning AI."

in simple terms, zero-shot prompting usually doesn‚Äôt include extra context or output indicators. You just give the instruction and the input, and the model tries to generate the output based on its training.
