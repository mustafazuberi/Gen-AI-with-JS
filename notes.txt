1 - What is Generative AI?

Generative AI (Gen AI) is a type of artificial intelligence designed to create new content. By content, we mean text, images, videos, voice, music, code, and more.

Traditionally, AI models were trained to analyze or classify data‚Äîfor example, detecting objects in images or categorizing text. Generative AI goes a step further: it generates new content that is similar to the data it was trained on, effectively creating something original while learning from existing patterns.

So inshort the generation of content using AI is called generative AI.

2 - What are LLMs?
LLMs, or Large Language Models, are AI models designed specifically for generating and understanding text. They can write, summarize, translate, or answer questions by learning patterns from vast amounts of text data.


3 ‚Äì What did we do before LLMs existed?

Before LLMs, we used statistical models. These models looked at the previous few words and guessed the next word using word frequency and probability. They were sometimes accurate, but very limited.

To improve, we started using neural networks and deep learning. One type we used was called Recurrent Neural Networks (RNNs). RNNs could generate better quality text than statistical models, but they had a limited memory. This meant they could only understand short contexts, not long ones, so they weren‚Äôt perfect.

Then, Google published a research paper called ‚ÄúAttention is All You Need‚Äù. They introduced a new architecture called the Transformer, which could look at all words in a sentence at the same time. This made text generation much more powerful and accurate.

Based on this research, new models were created, like GPT models and BERT models. The Transformer architecture became a game-changer in AI.

These models don‚Äôt just start predicting words on their own‚Äîthey are trained on massive amounts of data, like Wikipedia, books, and information available on the internet.

When we say a model has 8 billion parameters, or another has 32 billion, it refers to the weights of the neural network. More parameters usually mean the model is bigger and more powerful, while fewer parameters make it lighter and faster. Some of today‚Äôs state-of-the-art models are trained on trillions of parameters, which makes them extremely capable.


4 - Models and there capabilities?
There aren‚Äôt official categories for AI models, but we can group them to understand their capabilities:

1. GPT Models:
These models generate outputs immediately when they get input.
They don‚Äôt perform a step-by-step ‚Äúthinking‚Äù process.
This makes GPT models very fast at generating text.

2. Reasoning Models:
These models think through the problem before giving an output.
For example, if you ask the model to generate an image or solve a complex task, it analyzes the input step by step before producing the result.
Because of this thought process, reasoning models are slower than GPT models, but they can handle more complex tasks.

When to use which:
Just because GPT models are fast doesn‚Äôt mean we always use them.
For simple tasks, GPT models are fine.
For complex tasks or multi-step planning, reasoning models are better.
In agentic systems, sometimes a ‚Äúrouter‚Äù is used‚Äîa decision-making point where the model decides which path to take.
In these cases, the model needs reasoning power, so reasoning models are preferred.


5 - Distil Model?

A distil model is a smaller version of a big model.
The big model acts as a teacher, sharing its knowledge.
The small model learns from the big model, trying to copy its behavior.
This process of transferring knowledge is called distillation.

Why use it:
The small model is faster and lighter.
It can do most of what the big model does, but with less computing power.


6 - Tokens? 

A token is the smallest piece of text that an LLM can understand.
When you type something like:

üëâ "write me a poem"
It doesn‚Äôt go directly into the model. First, it passes through a step called tokenization.
A special tool called a tokenizer breaks the sentence into tokens.
A token can be a whole word or a part of a word (depends on how the tokenizer works).
Each token is then converted into a number (an ID).

Why? Because:
‚û°Ô∏è LLMs don‚Äôt understand text. They only understand numbers.
So, the text ‚Üí tokens ‚Üí numbers ‚Üí then embeddings ‚Üí then into the neural network.

Example:
"write me a poem" might become tokens like:
["write", " me", " a", " poem"]
and then numbers like:
[1509, 502, 257, 10503]

This is how your text is translated into something the model can actually process.

Why tokens matter:
We need to understand tokens because when we use APIs in Generative AI, the pricing is based on the number of tokens processed. Both the input tokens (your prompt) and the output tokens (the model‚Äôs reply) count toward usage. So, if a company says ‚Äú1 million tokens for $1,‚Äù it means that once your total processed tokens reach one million, you will be charged $1.



Context?
The surrounding text or information the model uses to understand and generate relevant responses.
Models do have a lot of knowledge, but they don‚Äôt have our personal information. For example, if we are building an agentic system for a company, the model doesn‚Äôt know the company‚Äôs internal information. We can provide that information to the model, and this approach is called RAG (Retrieval-Augmented Generation).

Along with that, there‚Äôs also something called message history. When we chat with an AI, by design the model is stateless‚Äîmeaning if we ask a question and it answers, it won‚Äôt remember that conversation in the next prompt. To solve this limitation, whenever we send input, we also include the message history.

All of this information we provide to the LLM is called context.


Context Window?
The maximum number of tokens an LLM can read and use at the same time to generate or predict text. An LLM also has limitations‚Äîit‚Äôs not like we can give it as much input as possible. The context size is called the context window, and it is determined by the token limitation we have. Just like that, every model has its own context window.

We always have to select models based on our requirements‚Äîhow much context size we need or how much output size we might require, since output tokens are also counted and limited. Another important term is knowledge cutoff. Whenever we use models, we often see in the documentation a mentioned date called the knowledge cutoff. This means that the model was trained on data up until that date, and after that point, it doesn‚Äôt have knowledge of new events or information.


Inference?
The process of an LLM taking input and producing output is called inference, and that‚Äôs the stage where users actually interact with the model. Training happens once (or in stages when models are fine-tuned), but inference happens millions or even billions of times as people use the model. That‚Äôs why today‚Äôs competition among cloud providers (like OpenAI, Anthropic, Google, Microsoft, Amazon, etc.) is largely about how fast, efficient, and cost-effective their inference is.

The race isn‚Äôt only about who has the biggest or most capable model‚Äîit‚Äôs also about who can deliver low-latency inference at scale (so users get instant responses without huge costs). This is why you see a lot of work around model optimization, GPU/TPU efficiency, quantization, batching, and distributed inference systems.

So when we select a model, we also need to consider its inference speed. If our application requires fast responses, we must ensure the model provides low-latency inference, which is usually mentioned in its specifications. Larger models typically take more time compared to smaller ones, and reasoning models can be slower as well because of the additional steps involved.


Prompt Engineering?
Prompt Engineering is one of the most important concepts in generative AI if we truly want to build agents and AI systems effectively.

The problem is that models are not deterministic, which means if we give the same input now, the output might be something different just a minute later. The output is not fixed, and the model does not always provide consistency. But when we build systems, we always need consistent outputs, because we don‚Äôt want to break other programming logic in our system.

Prompt engineering is simply a technique to improve the capacity of LLMs. A prompt is just the text we send to an LLM. For example, while using ChatGPT directly, the text we send is very simple. But when we use APIs, we often create prompts using a special structure with multiple elements:

Instructions: Tell the LLM what it needs to do (e.g., translation, summarization).
Input Data: The actual data on which the instruction should be performed.
Context: Additional information that helps the LLM understand better.
Output Indicator: Tells the LLM how or from where the output should start.

Example
Prompt:
You are an English grammar assistant. 
Fix grammar: "i is mustafa zuberi" 
Give only the corrected sentence as output.

Context ‚Üí You are an English grammar assistant.
Instruction ‚Üí Fix grammar
Input Data ‚Üí "i is mustafa zuberi"
Output Indicator ‚Üí Give only the corrected sentence as output.


Zero-shot prompting?
Zero-shot prompting is when we don‚Äôt give any examples to the LLM. We directly ask a question or give an instruction in the prompt, and the model tries to generate the answer based on its knowledge.

e.g: Translate this sentence into French: "I am learning AI."

in simple terms, zero-shot prompting usually doesn‚Äôt include extra context or output indicators. You just give the instruction and the input, and the model tries to generate the output based on its training.


LLM Settings?

1 - TEMPERATURE: The temperature setting controls how creative or deterministic the model‚Äôs responses are. A higher value (e.g., 0.8) makes the output more creative and varied, while a lower value (e.g., 0.2) makes it more focused and predictable. The choice depends on the use case‚Äîfor example, for content creation where creativity is needed, a higher temperature works better. In contrast, for decision-making tasks that require accuracy and consistency, a lower temperature is preferred.

2 - Top-p (nucleus sampling):
When the model picks the next word, every word has a chance (like tea 60%, coffee 30%, juice 8%, car 2%). All chances together = 100%.

Top-p says: ‚ÄúOnly keep the biggest words until their chances add up to a certain number.‚Äù

Example: top_p = 0.9 ‚Üí tea (60) + coffee (30) = 90%. Only tea and coffee are allowed.

top_p = 1.0 ‚Üí keep all words (tea, coffee, juice, car).
top_p = 0.1 ‚Üí keep just tea (already bigger than 10%).

üëâ Higher top_p = more creative (more words to choose).
üëâ Lower top_p = more focused (fewer words to choose).
üëâ Usually, you use either top_p or temperature to control randomness, not both.


EXAMPLE for top_p and temperature?

Step 1: The situation

We want to finish:
‚ÄúI want to drink a cup of ‚Ä¶‚Äù
The model thinks:
Tea is very likely
Coffee is also likely
Juice is a little likely
Car is super unlikely

Step 2: What Temperature does üé≤

Think of it like how seriously we take the model‚Äôs guess.
Low temperature ‚Üí always pick the safest guess (tea).
High temperature ‚Üí sometimes pick other guesses (coffee, juice, maybe even car).
üëâ Temperature = how risky or safe the choice is.

Step 3: What Top-p does ü™£
Think of it like crossing out bad options before choosing.
High top-p ‚Üí keep all options (tea, coffee, juice, car).
Lower top-p ‚Üí only keep the best options (maybe just tea + coffee).
Very low top-p ‚Üí keep only tea.
üëâ Top-p = how many options you allow.
Super short difference
Temperature = do we play safe or risky with the options?
Top-p = how many options do we even keep before picking?



3 - Stop: Tell the model to stop talking when it says a certain word.

4 - max_completion_tokens:  is a setting that limits how long the model‚Äôs output can be. A token is a small piece of text (it could be a word, part of a word, or punctuation), and roughly 1,000 tokens equal about 750 words in English. By setting this limit, you tell the model the maximum number of tokens it can generate in its response. For example, if you set max_completion_tokens: 100, the model will stop after about 75 words, giving you a short, focused reply. If you set max_completion_tokens: 1000, the model can produce a much longer answer, around 750 words. This is useful when you want to control cost, avoid overly long answers, or keep responses within a specific length. In simple words: it‚Äôs like telling the model, ‚ÄúYou can talk, but only this much and no more.‚Äù


5 - Presence penalty: Lowers the chance of saying the same word again at all. (One-time fixed penalty, no matter how many repeats.) 

7 - Frequency penalty: Lowers the chance more and more each time the word is repeated. Presence penalty = stop reuse Frequency penalty = stop overuse

Both presence penalty and frequency penalty work by reducing the probability of picking the repeated word again when the model chooses the next word.

Presence penalty: Drops the probability right away after the word shows up once.
Frequency penalty: Drops the probability a little, then more, then more‚Ä¶ the more times the word shows up.

Presence penalty = stop reuse (don‚Äôt repeat at all).
Frequency penalty = stop overuse (don‚Äôt repeat too much).

Structured Output?
Structured output in generative AI means instructing the model to produce responses that strictly follow a predefined format, usually JSON or another machine-readable structure, so the output can be directly used by software without extra parsing. This is done by either explicitly prompting the AI to respond in JSON, for example asking, ‚ÄúGenerate user info in JSON with name, email, and age,‚Äù which yields {"name":"Alice","email":"alice@example.com","age":25}, or by providing a full JSON schema that defines the object type, properties, and required fields, telling the AI to adhere strictly to it. This approach reduces ambiguity, prevents hallucinations in structured data, ensures consistency, and enables seamless integration into pipelines, applications, or databases, making AI output predictable, reliable, and ready for automated processing.


