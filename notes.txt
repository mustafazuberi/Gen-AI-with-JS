1 - What is Generative AI?

Generative AI (Gen AI) is a type of artificial intelligence designed to create new content. By content, we mean text, images, videos, voice, music, code, and more.

Traditionally, AI models were trained to analyze or classify data‚Äîfor example, detecting objects in images or categorizing text. Generative AI goes a step further: it generates new content that is similar to the data it was trained on, effectively creating something original while learning from existing patterns.

So inshort the generation of content using AI is called generative AI.

2 - What are LLMs?
LLMs, or Large Language Models, are AI models designed specifically for generating and understanding text. They can write, summarize, translate, or answer questions by learning patterns from vast amounts of text data.


3 ‚Äì What did we do before LLMs existed?

Before LLMs, we used statistical models. These models looked at the previous few words and guessed the next word using word frequency and probability. They were sometimes accurate, but very limited.

To improve, we started using neural networks and deep learning. One type we used was called Recurrent Neural Networks (RNNs). RNNs could generate better quality text than statistical models, but they had a limited memory. This meant they could only understand short contexts, not long ones, so they weren‚Äôt perfect.

Then, Google published a research paper called ‚ÄúAttention is All You Need‚Äù. They introduced a new architecture called the Transformer, which could look at all words in a sentence at the same time. This made text generation much more powerful and accurate.

Based on this research, new models were created, like GPT models and BERT models. The Transformer architecture became a game-changer in AI.

These models don‚Äôt just start predicting words on their own‚Äîthey are trained on massive amounts of data, like Wikipedia, books, and information available on the internet.

When we say a model has 8 billion parameters, or another has 32 billion, it refers to the weights of the neural network. More parameters usually mean the model is bigger and more powerful, while fewer parameters make it lighter and faster. Some of today‚Äôs state-of-the-art models are trained on trillions of parameters, which makes them extremely capable.


4 - Models and there capabilities?
There aren‚Äôt official categories for AI models, but we can group them to understand their capabilities:

1. GPT Models:
These models generate outputs immediately when they get input.
They don‚Äôt perform a step-by-step ‚Äúthinking‚Äù process.
This makes GPT models very fast at generating text.

2. Reasoning Models:
These models think through the problem before giving an output.
For example, if you ask the model to generate an image or solve a complex task, it analyzes the input step by step before producing the result.
Because of this thought process, reasoning models are slower than GPT models, but they can handle more complex tasks.

When to use which:
Just because GPT models are fast doesn‚Äôt mean we always use them.
For simple tasks, GPT models are fine.
For complex tasks or multi-step planning, reasoning models are better.
In agentic systems, sometimes a ‚Äúrouter‚Äù is used‚Äîa decision-making point where the model decides which path to take.
In these cases, the model needs reasoning power, so reasoning models are preferred.


5 - Distil Model?

A distil model is a smaller version of a big model.
The big model acts as a teacher, sharing its knowledge.
The small model learns from the big model, trying to copy its behavior.
This process of transferring knowledge is called distillation.

Why use it:
The small model is faster and lighter.
It can do most of what the big model does, but with less computing power.


6 - Tokens? 

A token is the smallest piece of text that an LLM can understand.
When you type something like:

üëâ "write me a poem"
It doesn‚Äôt go directly into the model. First, it passes through a step called tokenization.
A special tool called a tokenizer breaks the sentence into tokens.
A token can be a whole word or a part of a word (depends on how the tokenizer works).
Each token is then converted into a number (an ID).

Why? Because:
‚û°Ô∏è LLMs don‚Äôt understand text. They only understand numbers.
So, the text ‚Üí tokens ‚Üí numbers ‚Üí then embeddings ‚Üí then into the neural network.

Example:
"write me a poem" might become tokens like:
["write", " me", " a", " poem"]
and then numbers like:
[1509, 502, 257, 10503]

This is how your text is translated into something the model can actually process.

Why tokens matter:
We need to understand tokens because when we use APIs in Generative AI, the pricing is based on the number of tokens processed. Both the input tokens (your prompt) and the output tokens (the model‚Äôs reply) count toward usage. So, if a company says ‚Äú1 million tokens for $1,‚Äù it means that once your total processed tokens reach one million, you will be charged $1.



Context?
The surrounding text or information the model uses to understand and generate relevant responses.
Models do have a lot of knowledge, but they don‚Äôt have our personal information. For example, if we are building an agentic system for a company, the model doesn‚Äôt know the company‚Äôs internal information. We can provide that information to the model, and this approach is called RAG (Retrieval-Augmented Generation).

Along with that, there‚Äôs also something called message history. When we chat with an AI, by design the model is stateless‚Äîmeaning if we ask a question and it answers, it won‚Äôt remember that conversation in the next prompt. To solve this limitation, whenever we send input, we also include the message history.

All of this information we provide to the LLM is called context.
